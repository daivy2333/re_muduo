# Muduo网络库性能测试指南

## 目录
1. [测试环境准备](#测试环境准备)
2. [测试程序说明](#测试程序说明)
3. [测试参数配置](#测试参数配置)
4. [测试执行步骤](#测试执行步骤)
5. [结果分析](#结果分析)
6. [注意事项](#注意事项)

---

## 测试环境准备

### 系统参数调优
在进行极限测试前，需要调整系统参数：

```bash
sudo sysctl -a > /tmp/sysctl_backup.conf
# 增加文件描述符限制
ulimit -n 65535

# 调整TCP参数
sudo sysctl -w net.core.somaxconn=65535
sudo sysctl -w net.ipv4.tcp_max_syn_backlog=65535
sudo sysctl -w net.ipv4.tcp_tw_reuse=1
sudo sysctl -w net.ipv4.tcp_fin_timeout=30
sudo sysctl -w net.core.netdev_max_backlog=30000




### 编译测试程序
```bash
cd benchmark
make


结束后

sudo sysctl -p /tmp/sysctl_backup.conf
```

---

## 测试程序说明

### 1. latency_test.cpp - 延迟测试
测试不同负载大小下的延迟分布（P50/P95/P99）。

**特点：**
- 专注于延迟指标
- 支持多种payload大小
- 记录详细的延迟统计

### 2. echo_server_bench.cpp - 回显服务器基准测试
测试服务器的吞吐量和并发处理能力。

**特点：**
- 固定payload大小
- 测试QPS和吞吐量
- 评估并发性能

### 3. self_stress_test.cpp - 自压测试
综合测试服务器的连接处理能力和稳定性。

**特点：**
- 支持大量并发连接
- 实时显示连接状态
- 记录失败请求数

---

## 测试参数配置

### latency_test.cpp 测试参数

#### 基础挡位（预热测试）
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 1    | 64B        | 10      | 10s     | 4          |
| 2    | 256B       | 10      | 10s     | 4          |
| 3    | 1024B      | 10      | 10s     | 4          |
| 4    | 4096B      | 10      | 10s     | 4          |

#### 中等负载挡位
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 5    | 64B        | 100     | 30s     | 4          |
| 6    | 256B       | 100     | 30s     | 4          |
| 7    | 1024B      | 100     | 30s     | 4          |
| 8    | 4096B      | 100     | 30s     | 4          |

#### 高负载挡位
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 9    | 64B        | 500     | 30s     | 8          |
| 10   | 256B       | 500     | 30s     | 8          |
| 11   | 1024B      | 500     | 30s     | 8          |
| 12   | 4096B      | 500     | 30s     | 8          |

#### 极限负载挡位
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 13   | 64B        | 1000    | 60s     | 8          |
| 14   | 256B       | 1000    | 60s     | 8          |
| 15   | 1024B      | 1000    | 60s     | 8          |
| 16   | 4096B      | 1000    | 60s     | 8          |
| 17   | 16384B     | 1000    | 60s     | 8          |
| 18   | 65536B     | 1000    | 60s     | 8          |

### echo_server_bench.cpp 测试参数

#### 基础挡位
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 1    | 1024B      | 10      | 10s     | 4          |
| 2    | 1024B      | 50      | 10s     | 4          |
| 3    | 1024B      | 100     | 10s     | 4          |

#### 中等负载挡位
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 4    | 1024B      | 200     | 30s     | 4          |
| 5    | 2048B      | 200     | 30s     | 4          |
| 6    | 4096B      | 200     | 30s     | 4          |

#### 高负载挡位
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 7    | 4096B      | 500     | 30s     | 8          |
| 8    | 8192B      | 500     | 30s     | 8          |
| 9    | 16384B     | 500     | 30s     | 8          |

#### 极限负载挡位
| 挡位 | Payload大小 | 客户端数 | 持续时间 | 服务器线程数 |
|------|------------|---------|---------|------------|
| 10   | 16384B     | 1000    | 60s     | 8          |
| 11   | 32768B     | 1000    | 60s     | 8          |
| 12   | 65536B     | 1000    | 60s     | 8          |

### self_stress_test.cpp 测试参数

#### 基础挡位
| 挡位 | 客户端数 | Payload大小 | 持续时间 | 服务器线程数 |
|------|---------|------------|---------|------------|
| 1    | 10      | 1024B      | 10s     | 4          |
| 2    | 50      | 1024B      | 10s     | 4          |
| 3    | 100     | 1024B      | 10s     | 4          |

#### 中等负载挡位
| 挡位 | 客户端数 | Payload大小 | 持续时间 | 服务器线程数 |
|------|---------|------------|---------|------------|
| 4    | 200     | 1024B      | 30s     | 4          |
| 5    | 500     | 1024B      | 30s     | 4          |
| 6    | 1000    | 1024B      | 30s     | 4          |

#### 高负载挡位
| 挡位 | 客户端数 | Payload大小 | 持续时间 | 服务器线程数 |
|------|---------|------------|---------|------------|
| 7    | 2000    | 1024B      | 30s     | 8          |
| 8    | 3000    | 1024B      | 30s     | 8          |
| 9    | 5000    | 1024B      | 30s     | 8          |

#### 极限负载挡位
| 挡位 | 客户端数 | Payload大小 | 持续时间 | 服务器线程数 |
|------|---------|------------|---------|------------|
| 10   | 10000   | 1024B      | 60s     | 8          |
| 11   | 10000   | 4096B      | 60s     | 8          |
| 12   | 10000   | 16384B     | 60s     | 8          |

---

## 测试执行步骤

### 1. latency_test.cpp 执行步骤

```bash
# 修改代码中的参数
vim benchmark/latency_test.cpp

# 修改以下参数（以挡位5为例）：
# 第178行：std::vector<int> payload_sizes = {64};
# 第179行：const int clients = 100;
# 第180行：const int duration = 30;
# 第45行：server_->setThreadNum(4);

# 编译并运行
cd benchmark
make
./latency_test
```

### 2. echo_server_bench.cpp 执行步骤

```bash
# 需要修改代码中的参数
vim benchmark/echo_server_bench.cpp

# 修改以下参数（以挡位4为例）：
# 第42行：server_->setThreadNum(4);
# 构造函数参数和run()参数需要在调用处修改

# 编译并运行
cd benchmark
make
./echo_server_bench
```

### 3. self_stress_test.cpp 执行步骤

```bash
# 修改代码中的参数
vim benchmark/self_stress_test.cpp

# 修改以下参数（以挡位4为例）：
# 第136行：const int payload_size = 1024;
# 第78行：server_->setThreadNum(4);

# 编译并运行
cd benchmark
make
./self_stress_test 8888 200 30
```

---

## 结果分析

### 关键指标说明

1. **延迟指标（latency_test）**
   - Avg Latency: 平均延迟
   - P50 Latency: 50分位延迟
   - P95 Latency: 95分位延迟
   - P99 Latency: 99分位延迟

2. **吞吐量指标（echo_server_bench）**
   - QPS: 每秒查询数
   - Throughput: 吞吐量（MB/s）

3. **连接指标（self_stress_test）**
   - Total Connections: 总连接数
   - Active Connections: 活跃连接数
   - Failed Requests: 失败请求数

### 结果记录模板

```markdown
## 测试结果记录

### 测试日期：YYYY-MM-DD
### 测试环境：CPU/内存/网络

#### latency_test 结果
| 挡位 | Payload | 客户端数 | Avg延迟 | P50延迟 | P95延迟 | P99延迟 |
|------|---------|---------|---------|---------|---------|---------|
| 1    | 64B     | 10      | ...     | ...     | ...     | ...     |
| 2    | 256B    | 10      | ...     | ...     | ...     | ...     |

#### echo_server_bench 结果
| 挡位 | Payload | 客户端数 | QPS     | 吞吐量  |
|------|---------|---------|---------|---------|
| 1    | 1024B   | 10      | ...     | ...     |
| 2    | 1024B   | 50      | ...     | ...     |

#### self_stress_test 结果
| 挡位 | 客户端数 | Payload | QPS     | 失败请求 | 吞吐量  |
|------|---------|---------|---------|---------|---------|
| 1    | 10      | 1024B   | ...     | ...     | ...     |
| 2    | 50      | 1024B   | ...     | ...     | ...     |
```

---

## 性能极限说明

### 当前参数配置

#### latency_test.cpp
- 服务器线程数：8
- 数据包大小：64B ~ 65536B
- 并发客户端数：1000
- 测试持续时间：60秒

#### throughput_test.cpp
- 客户端数量：10 ~ 2000
- 数据包大小：64B ~ 65536B
- 测试持续时间：30秒

#### self_stress_test.cpp
- 服务器线程数：8
- 数据包大小：16384B
- 客户端数量：5000
- 测试持续时间：60秒

### 性能极限评估

当前参数配置**不是**性能极限，仍可进一步上调。以下是可以调整的方向：

#### 1. 线程数调整
- 当前：8个线程
- 可调整至：16~32个线程（取决于CPU核心数）
- 建议：设置为CPU核心数的2倍左右

#### 2. 客户端数量调整
- latency_test：当前1000，可调整至5000~10000
- throughput_test：当前2000，可调整至5000~10000
- self_stress_test：当前5000，可调整至10000~50000
- 限制：需要相应增加文件描述符限制（ulimit -n）

#### 3. 数据包大小调整
- 当前最大：65536B（64KB）
- 可调整至：1MB甚至更大
- 注意：大包测试需要更多内存，可能导致内存瓶颈

#### 4. 测试持续时间调整
- 当前：30~60秒
- 可调整至：5~10分钟
- 建议：长时间测试可以发现内存泄漏等稳定性问题

### 系统资源限制

在调高参数前，需要考虑以下系统限制：

#### 文件描述符限制
```bash
# 查看当前限制
ulimit -n
# 临时增加
ulimit -n 1000000
# 永久增加（编辑/etc/security/limits.conf）
* soft nofile 1000000
* hard nofile 1000000
```

#### TCP连接限制
```bash
# 增加TCP连接队列
sudo sysctl -w net.core.somaxconn=65535
sudo sysctl -w net.ipv4.tcp_max_syn_backlog=65535

# 增加本地端口范围
sudo sysctl -w net.ipv4.ip_local_port_range="10000 65000"
```

#### 内存限制
- 每个连接需要一定内存（缓冲区等）
- 50000个连接可能需要数GB内存
- 建议监控内存使用情况，避免OOM

### 实际极限测试建议

如果要进行真正的极限测试，建议按以下步骤进行：

1. **逐步增加**：不要一次性设置到最大值
2. **监控指标**：CPU、内存、网络、错误率
3. **记录瓶颈**：记录性能开始下降的临界点
4. **稳定性测试**：在接近极限的参数下运行较长时间（如1小时）

---

## 注意事项

### 测试前
1. 确保系统资源充足（CPU、内存、网络）
2. 关闭不必要的后台程序
3. 检查系统参数是否正确配置
4. 确保测试程序已正确编译

### 测试中
1. 监控系统资源使用情况
2. 观察是否有错误日志输出
3. 记录异常现象
4. 每个挡位建议测试2-3次取平均值

### 测试后
1. 保存测试结果
2. 分析性能瓶颈
3. 对比不同挡位的性能变化
4. 记录系统在极限负载下的表现

### 常见问题
1. **连接失败**：检查文件描述符限制和系统TCP参数
2. **延迟过高**：检查系统负载和网络状况
3. **内存溢出**：减少并发客户端数或增加系统内存
4. **CPU占用过高**：这是正常现象，属于极限测试的预期结果
